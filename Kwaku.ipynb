{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c76d9b28",
    "outputId": "dfa3cb53-7de4-413c-c74b-a14335d8d7c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m235.5/253.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-docx\n",
      "Successfully installed python-docx-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6s-QpJL1ozp2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "568bu_L3o0tW",
    "outputId": "00d48756-28b0-4b71-db0b-3543433b0f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 458 students\n",
      "SHAP shape mismatch: (115, 10, 2) vs (115, 10)\n",
      "Force plot failed: In v0.20, force plot now requires the base value as the first parameter! Try shap.plots.force(explainer.expected_value, shap_values) or for multi-output models try shap.plots.force(explainer.expected_value[0], shap_values[..., 0]).\n",
      "Report saved to /content/Ledzokuku_JHS_Analysis_Report.docx\n",
      "Error saving processed data: Expected a 1D array, got an array with shape (458, 2)\n",
      "Analysis complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# JHS Student Performance Analysis with Word Report Generation\n",
    "# Features:\n",
    "# - Predicts if average score will be <50 or >=50\n",
    "# - Maintains all original performance categorization\n",
    "# - Generates comprehensive Word report\n",
    "# - Includes SHAP values for model interpretability\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                           confusion_matrix, roc_auc_score)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optional SHAP import for feature explanation\n",
    "try:\n",
    "    import shap\n",
    "    _HAS_SHAP = True\n",
    "except ImportError:\n",
    "    _HAS_SHAP = False\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIGURATION\n",
    "# ==========================================================\n",
    "\n",
    "# File paths\n",
    "FILE_PATH = \"/content/CAPSTONE DATA.xlsx\"\n",
    "OUTPUT_WORD = \"/content/Ledzokuku_JHS_Analysis_Report.docx\"\n",
    "OUTPUT_PROCESSED = \"/content/Ledzokuku_JHS_Processed.xlsx\"\n",
    "FIG_DIR = \"/content/figures\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# Performance thresholds\n",
    "PASS_THRESHOLD = 50\n",
    "AT_RISK_LOW = PASS_THRESHOLD\n",
    "AT_RISK_HIGH = PASS_THRESHOLD + 4  # 50-54 range\n",
    "MEETS_EXPECTATIONS_THRESHOLD = 70\n",
    "HIGH_ACHIEVER_THRESHOLD = 80\n",
    "\n",
    "# Expected subject names (with flexible naming)\n",
    "EXPECTED_SUBJECTS = ['ENG', 'MATHS', 'SCI', 'SOC', 'CAREER',\n",
    "                     'COMPUTING', 'RME', 'C A', 'GL', 'FREN']\n",
    "\n",
    "# Visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# ==========================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================================\n",
    "\n",
    "def clean_column_name(col):\n",
    "    \"\"\"Standardize column names by removing spaces and special chars\"\"\"\n",
    "    return str(col).strip().replace('.', '').replace(' ', '_')\n",
    "\n",
    "def save_figure(fname):\n",
    "    \"\"\"Save matplotlib figure to file with consistent formatting\"\"\"\n",
    "    path = os.path.join(FIG_DIR, fname)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "def add_dataframe_to_doc(doc, df, title=None, max_rows=30):\n",
    "    \"\"\"\n",
    "    Insert a pandas DataFrame into Word document as a formatted table\n",
    "    Handles large tables by truncating rows and columns\n",
    "    \"\"\"\n",
    "    if title:\n",
    "        paragraph = doc.add_paragraph()\n",
    "        paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n",
    "        paragraph.add_run(title).bold = True\n",
    "\n",
    "    # Truncate if too large\n",
    "    df_display = df.copy()\n",
    "    if df_display.shape[0] > max_rows:\n",
    "        df_display = df_display.head(max_rows)\n",
    "\n",
    "    max_cols = 12\n",
    "    if df_display.shape[1] > max_cols:\n",
    "        cols = list(df_display.columns[:max_cols-2]) + list(df_display.columns[-2:])\n",
    "        df_display = df_display[cols]\n",
    "\n",
    "    # Create table\n",
    "    table = doc.add_table(rows=1, cols=len(df_display.columns))\n",
    "    table.style = 'Light Shading Accent 1'\n",
    "\n",
    "    # Add headers\n",
    "    header_cells = table.rows[0].cells\n",
    "    for j, col in enumerate(df_display.columns):\n",
    "        header_cells[j].text = str(col)\n",
    "\n",
    "    # Add data rows\n",
    "    for _, row in df_display.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for j, col in enumerate(df_display.columns):\n",
    "            value = row[col]\n",
    "            row_cells[j].text = \"\" if pd.isna(value) else str(value)\n",
    "\n",
    "    doc.add_paragraph()\n",
    "\n",
    "# ==========================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ==========================================================\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load Excel data and perform initial cleaning and standardization\n",
    "    Returns cleaned DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"Failed to load {file_path}: {e}\")\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = [clean_column_name(c) for c in df.columns]\n",
    "\n",
    "    # Handle common subject name variations\n",
    "    name_mapping = {\n",
    "        'MATH': 'MATHS',\n",
    "        'COM': 'COMPUTING',\n",
    "        'GH_LAN': 'GL',\n",
    "        'CAR_TECH': 'CAREER',\n",
    "        'C A': 'C_A'  # Standardize to underscore format\n",
    "    }\n",
    "\n",
    "    # Apply name mapping\n",
    "    df.rename(columns=name_mapping, inplace=True)\n",
    "\n",
    "    # Convert numeric columns, coerce errors to NaN\n",
    "    for col in df.columns:\n",
    "        if col in [s.replace(' ', '_') for s in EXPECTED_SUBJECTS]:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Remove completely empty rows\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Ensure scores are within 0-100 range\n",
    "    for subject in [s.replace(' ', '_') for s in EXPECTED_SUBJECTS]:\n",
    "        if subject in df.columns:\n",
    "            df[subject] = df[subject].clip(lower=0, upper=100)\n",
    "\n",
    "    # Calculate average score\n",
    "    valid_subjects = [s.replace(' ', '_') for s in EXPECTED_SUBJECTS\n",
    "                     if s.replace(' ', '_') in df.columns and\n",
    "                     pd.api.types.is_numeric_dtype(df[s.replace(' ', '_')])]\n",
    "    df['Average_Score'] = df[valid_subjects].mean(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load and preprocess data\n",
    "df = load_and_preprocess_data(FILE_PATH)\n",
    "print(f\"Loaded dataset with {len(df)} students\")\n",
    "\n",
    "# ==========================================================\n",
    "# SUBJECT ANALYSIS\n",
    "# ==========================================================\n",
    "\n",
    "def analyze_subject_performance(df, subjects):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics for each subject:\n",
    "    - Average and median scores\n",
    "    - Pass rates\n",
    "    - At-risk rates\n",
    "    - Excellence rates\n",
    "    Returns DataFrame with subject metrics\n",
    "    \"\"\"\n",
    "    subject_metrics = {}\n",
    "\n",
    "    for subject in subjects:\n",
    "        if subject in df.columns and pd.api.types.is_numeric_dtype(df[subject]):\n",
    "            scores = df[subject]\n",
    "\n",
    "            subject_metrics[subject] = {\n",
    "                'Average_Score': scores.mean(),\n",
    "                'Median_Score': scores.median(),\n",
    "                f'Pass_Rate_>={PASS_THRESHOLD}%': (scores >= PASS_THRESHOLD).mean() * 100,\n",
    "                f'At_Risk_Rate_{AT_RISK_LOW}-{AT_RISK_HIGH}%': ((scores >= AT_RISK_LOW) & (scores <= AT_RISK_HIGH)).mean() * 100,\n",
    "                f'Critical_Fail_Rate_<{PASS_THRESHOLD}%': (scores < PASS_THRESHOLD).mean() * 100,\n",
    "                f'Excellence_Rate_>={HIGH_ACHIEVER_THRESHOLD}%': (scores >= HIGH_ACHIEVER_THRESHOLD).mean() * 100\n",
    "            }\n",
    "\n",
    "    metrics_df = pd.DataFrame.from_dict(subject_metrics, orient='index')\n",
    "    return metrics_df.sort_values('Average_Score', ascending=False)\n",
    "\n",
    "# Identify valid subjects for analysis\n",
    "valid_subjects = [s.replace(' ', '_') for s in EXPECTED_SUBJECTS\n",
    "                 if s.replace(' ', '_') in df.columns and\n",
    "                 pd.api.types.is_numeric_dtype(df[s.replace(' ', '_')])]\n",
    "\n",
    "# Perform subject analysis\n",
    "subject_metrics = analyze_subject_performance(df, valid_subjects)\n",
    "\n",
    "# ==========================================================\n",
    "# STUDENT CATEGORIZATION\n",
    "# ==========================================================\n",
    "\n",
    "def categorize_students(df):\n",
    "    \"\"\"\n",
    "    Categorize students based on performance thresholds\n",
    "    Adds columns to DataFrame and returns summary counts\n",
    "    \"\"\"\n",
    "    # Initialize category columns\n",
    "    categories = {\n",
    "        'Critical_Fail': 0,\n",
    "        'At_Risk': 0,\n",
    "        'Meets_Expectations': 0,\n",
    "        'High_Achiever': 0\n",
    "    }\n",
    "\n",
    "    # Check if core subjects are available\n",
    "    core_subjects = ['ENG', 'MATHS', 'SCI']\n",
    "    has_core = all(s in df.columns for s in core_subjects)\n",
    "\n",
    "    if has_core:\n",
    "        # Categorize based on core subjects\n",
    "        df['Critical_Fail'] = ((df['ENG'] < PASS_THRESHOLD) |\n",
    "                              (df['MATHS'] < PASS_THRESHOLD) |\n",
    "                              (df['SCI'] < PASS_THRESHOLD)).astype(int)\n",
    "\n",
    "        df['At_Risk'] = (((df['ENG'] >= AT_RISK_LOW) & (df['ENG'] <= AT_RISK_HIGH)) |\n",
    "                         ((df['MATHS'] >= AT_RISK_LOW) & (df['MATHS'] <= AT_RISK_HIGH)) |\n",
    "                         ((df['SCI'] >= AT_RISK_LOW) & (df['SCI'] <= AT_RISK_HIGH))).astype(int)\n",
    "\n",
    "        df['Meets_Expectations'] = ((df['ENG'] >= MEETS_EXPECTATIONS_THRESHOLD) &\n",
    "                                   (df['MATHS'] >= MEETS_EXPECTATIONS_THRESHOLD) &\n",
    "                                   (df['SCI'] >= MEETS_EXPECTATIONS_THRESHOLD)).astype(int)\n",
    "\n",
    "        df['High_Achiever'] = ((df['ENG'] >= HIGH_ACHIEVER_THRESHOLD) &\n",
    "                              (df['MATHS'] >= HIGH_ACHIEVER_THRESHOLD) &\n",
    "                              (df['SCI'] >= HIGH_ACHIEVER_THRESHOLD)).astype(int)\n",
    "    else:\n",
    "        # Fallback to average score if available\n",
    "        if 'Average_Score' in df.columns:\n",
    "            df['Critical_Fail'] = (df['Average_Score'] < PASS_THRESHOLD).astype(int)\n",
    "            df['At_Risk'] = ((df['Average_Score'] >= AT_RISK_LOW) &\n",
    "                            (df['Average_Score'] <= AT_RISK_HIGH)).astype(int)\n",
    "            df['Meets_Expectations'] = (df['Average_Score'] >= MEETS_EXPECTATIONS_THRESHOLD).astype(int)\n",
    "            df['High_Achiever'] = (df['Average_Score'] >= HIGH_ACHIEVER_THRESHOLD).astype(int)\n",
    "\n",
    "    # Calculate summary counts\n",
    "    total_students = len(df)\n",
    "    summary = {\n",
    "        'Critical_Fail': int(df['Critical_Fail'].sum()),\n",
    "        'At_Risk': int(df['At_Risk'].sum()),\n",
    "        'Meets_Expectations': int(df['Meets_Expectations'].sum()),\n",
    "        'High_Achiever': int(df['High_Achiever'].sum()),\n",
    "        'Total_Students': total_students\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Categorize students and get summary\n",
    "student_summary = categorize_students(df)\n",
    "\n",
    "# ==========================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ==========================================================\n",
    "\n",
    "def plot_subject_averages(metrics_df):\n",
    "    \"\"\"Create bar plot of subject average scores with threshold lines\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(x=metrics_df.index, y='Average_Score', data=metrics_df.reset_index())\n",
    "\n",
    "    # Add threshold lines\n",
    "    plt.axhline(y=HIGH_ACHIEVER_THRESHOLD, color='gold', linestyle=':',\n",
    "               label=f'High ({HIGH_ACHIEVER_THRESHOLD})')\n",
    "    plt.axhline(y=MEETS_EXPECTATIONS_THRESHOLD, color='green', linestyle='--',\n",
    "               label=f'Meets ({MEETS_EXPECTATIONS_THRESHOLD})')\n",
    "    plt.axhline(y=PASS_THRESHOLD, color='orange', linestyle='-',\n",
    "               label=f'Pass ({PASS_THRESHOLD})')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.title('Subject Average Scores with Performance Thresholds')\n",
    "    plt.legend()\n",
    "\n",
    "    return save_figure('subject_averages.png')\n",
    "\n",
    "def plot_core_correlations(df):\n",
    "    \"\"\"Create heatmap of correlations between core subjects\"\"\"\n",
    "    core_subjects = [s for s in ['ENG', 'MATHS', 'SCI', 'GL', 'STEM_Score', 'Core_Avg']\n",
    "                    if s in df.columns and pd.api.types.is_numeric_dtype(df[s])]\n",
    "\n",
    "    if len(core_subjects) >= 2:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(df[core_subjects].corr(), annot=True, fmt='.2f',\n",
    "                   cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Core Subject Correlations')\n",
    "        return save_figure('core_correlations.png')\n",
    "    return None\n",
    "\n",
    "# Generate visualizations\n",
    "subject_avg_plot = plot_subject_averages(subject_metrics)\n",
    "core_corr_plot = plot_core_correlations(df)\n",
    "\n",
    "# ==========================================================\n",
    "# MODIFIED MACHINE LEARNING ANALYSIS (Predicting Average Score <50)\n",
    "# ==========================================================\n",
    "\n",
    "def train_average_score_model(df):\n",
    "    \"\"\"\n",
    "    Train model to predict if average score will be <50 or >=50\n",
    "    Returns:\n",
    "    - Trained model\n",
    "    - Results dictionary\n",
    "    - Confusion matrix DataFrame\n",
    "    - Feature importances\n",
    "    - SHAP values and plots (if available)\n",
    "    \"\"\"\n",
    "    # Create target variable (1 if average score <50, else 0)\n",
    "    if 'Average_Score' not in df.columns:\n",
    "        return None, {'note': 'Average_Score not available'}, None, None, None\n",
    "\n",
    "    y = (df['Average_Score'] < PASS_THRESHOLD).astype(int)\n",
    "\n",
    "    # Skip if not enough failing examples\n",
    "    if y.sum() < 5:\n",
    "        return None, {'note': f'Insufficient students with average <{PASS_THRESHOLD}'}, None, None, None\n",
    "\n",
    "    # Select features (all subject scores)\n",
    "    features = [s for s in valid_subjects if s in df.columns]\n",
    "\n",
    "    if len(features) < 2:\n",
    "        return None, {'note': 'Insufficient subjects for modeling'}, None, None, None\n",
    "\n",
    "    X = df[features].fillna(df[features].median())\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    calibrated_model = CalibratedClassifierCV(model, cv=5, method='isotonic')\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = calibrated_model.predict(X_test)\n",
    "    y_proba = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                       index=['Actual >=50', 'Actual <50'],\n",
    "                       columns=['Predicted >=50', 'Predicted <50'])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix for Average Score Prediction')\n",
    "    cm_plot = save_figure('confusion_matrix_avg_score.png')\n",
    "\n",
    "    # Get feature importances\n",
    "    importance_model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    importance_model.fit(X_train, y_train)\n",
    "    feature_importances = pd.Series(\n",
    "        importance_model.feature_importances_,\n",
    "        index=features\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=feature_importances.values, y=feature_importances.index)\n",
    "    plt.title('Feature Importances for Average Score Prediction')\n",
    "    fi_plot = save_figure('feature_importance_avg_score.png')\n",
    "\n",
    "    # SHAP Analysis - FIXED VERSION\n",
    "    shap_results = {}\n",
    "    if _HAS_SHAP:\n",
    "        try:\n",
    "            # Create SHAP explainer using the trained model\n",
    "            explainer = shap.TreeExplainer(importance_model)\n",
    "\n",
    "            # Calculate SHAP values - ensure we're using the right class\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "            # Handle binary classification SHAP values structure\n",
    "            if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "                # For binary classification, shap_values is a list of [class0, class1]\n",
    "                shap_values_class1 = shap_values[1]  # Use class 1 (average <50)\n",
    "            else:\n",
    "                # For some versions, it might be a single array\n",
    "                shap_values_class1 = shap_values\n",
    "\n",
    "            # Ensure the shape matches\n",
    "            if shap_values_class1.shape != X_test.shape:\n",
    "                print(f\"SHAP shape mismatch: {shap_values_class1.shape} vs {X_test.shape}\")\n",
    "                # Try to handle common shape issues\n",
    "                if len(shap_values_class1.shape) == 3:\n",
    "                    shap_values_class1 = shap_values_class1[:, :, 1]  # For 3D arrays\n",
    "                elif len(shap_values_class1.shape) == 1:\n",
    "                    shap_values_class1 = shap_values_class1.reshape(-1, 1)\n",
    "\n",
    "            # Summary plot with proper shape handling\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values_class1, X_test, feature_names=features, show=False)\n",
    "            plt.title('SHAP Summary Plot - Impact on Predicting Average <50')\n",
    "            shap_summary_plot = save_figure('shap_summary_plot.png')\n",
    "\n",
    "            # Force plot for a typical example with proper error handling\n",
    "            try:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                # Use the first sample that's correctly predicted for better demonstration\n",
    "                correct_predictions = (y_pred == y_test)\n",
    "                if correct_predictions.any():\n",
    "                    sample_idx = np.where(correct_predictions)[0][0]\n",
    "                else:\n",
    "                    sample_idx = 0\n",
    "\n",
    "                shap.force_plot(explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n",
    "                              shap_values_class1[sample_idx, :],\n",
    "                              X_test.iloc[sample_idx, :],\n",
    "                              feature_names=features,\n",
    "                              matplotlib=True,\n",
    "                              show=False)\n",
    "                plt.title('SHAP Force Plot - Individual Prediction Explanation')\n",
    "                shap_force_plot = save_figure('shap_force_plot.png')\n",
    "            except Exception as force_error:\n",
    "                print(f\"Force plot failed: {force_error}\")\n",
    "                shap_force_plot = None\n",
    "\n",
    "            # Dependence plot for most important feature\n",
    "            try:\n",
    "                most_important_feature = feature_importances.index[0]\n",
    "                feature_idx = list(features).index(most_important_feature)\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                shap.dependence_plot(feature_idx, shap_values_class1, X_test,\n",
    "                                   feature_names=features, show=False)\n",
    "                plt.title(f'SHAP Dependence Plot - {most_important_feature}')\n",
    "                shap_dependence_plot = save_figure('shap_dependence_plot.png')\n",
    "            except Exception as dep_error:\n",
    "                print(f\"Dependence plot failed: {dep_error}\")\n",
    "                shap_dependence_plot = None\n",
    "\n",
    "            shap_results = {\n",
    "                'shap_values': shap_values_class1,\n",
    "                'explainer': explainer,\n",
    "                'summary_plot': shap_summary_plot,\n",
    "                'force_plot': shap_force_plot,\n",
    "                'dependence_plot': shap_dependence_plot,\n",
    "                'expected_value': explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP analysis failed: {e}\")\n",
    "            shap_results = {'error': str(e)}\n",
    "\n",
    "    # Store results\n",
    "    results = {\n",
    "        'features': features,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'confusion_matrix': cm_df,\n",
    "        'confusion_matrix_plot': cm_plot,\n",
    "        'feature_importance_plot': fi_plot,\n",
    "        'feature_importances': feature_importances,\n",
    "        'shap_results': shap_results\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        results['auc'] = roc_auc_score(y_test, y_proba)\n",
    "    except:\n",
    "        results['auc'] = None\n",
    "\n",
    "    # Add probabilities to dataframe\n",
    "    df['Avg_Score_Fail_Probability'] = calibrated_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return calibrated_model, results, cm_df, feature_importances, shap_results\n",
    "\n",
    "# Train model for average score prediction\n",
    "avg_score_model, ml_results, confusion_matrix, feature_importances, shap_results = train_average_score_model(df)\n",
    "\n",
    "# ==========================================================\n",
    "# REPORT GENERATION\n",
    "# ==========================================================\n",
    "\n",
    "def get_subject_recommendations(subject):\n",
    "    \"\"\"Return practical recommendations for each subject\"\"\"\n",
    "    subject = subject.upper()\n",
    "\n",
    "    if subject.startswith('ENG'):\n",
    "        return [\n",
    "            \"Daily 30-minute reading comprehension exercises\",\n",
    "            \"Weekly vocabulary building with contextual usage\",\n",
    "            \"Peer-assisted learning for struggling readers\",\n",
    "            \"Structured writing practice with rubrics\"\n",
    "        ]\n",
    "    elif subject.startswith('MATH'):\n",
    "        return [\n",
    "            \"Daily basic arithmetic drills (15 minutes)\",\n",
    "            \"Hands-on activities with real-world applications\",\n",
    "            \"Small-group problem solving sessions\",\n",
    "            \"Weekly formative assessments with immediate feedback\"\n",
    "        ]\n",
    "    elif subject.startswith('SCI'):\n",
    "        return [\n",
    "            \"Practical experiments using local materials\",\n",
    "            \"Concept mapping for key topics\",\n",
    "            \"Relate concepts to everyday phenomena\",\n",
    "            \"Structured note-taking practice\"\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            \"Targeted vocabulary building\",\n",
    "            \"Structured reading comprehension practice\",\n",
    "            \"Peer discussion groups\",\n",
    "            \"Weekly short-answer assessments\"\n",
    "        ]\n",
    "\n",
    "def generate_word_report():\n",
    "    \"\"\"Create comprehensive Word report with all analysis results\"\"\"\n",
    "    doc = Document()\n",
    "\n",
    "    # Title page\n",
    "    doc.add_heading('Ledzokuku Municipality — JHS Student Performance Analysis', level=1)\n",
    "    doc.add_paragraph(f\"Data file: {os.path.basename(FILE_PATH)}\")\n",
    "    doc.add_paragraph(f\"Analysis date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "    doc.add_paragraph(\n",
    "        f\"Thresholds: PASS={PASS_THRESHOLD} | AT-RISK={AT_RISK_LOW}-{AT_RISK_HIGH} | \"\n",
    "        f\"MEETS={MEETS_EXPECTATIONS_THRESHOLD} | HIGH={HIGH_ACHIEVER_THRESHOLD}\"\n",
    "    )\n",
    "\n",
    "    # 1. Executive Summary\n",
    "    doc.add_heading('1. Executive Summary', level=2)\n",
    "    doc.add_paragraph(\n",
    "        f\"This report analyzes performance data for {student_summary['Total_Students']} JHS students. \"\n",
    "        f\"Key findings include {student_summary['Critical_Fail']} students below passing threshold \"\n",
    "        f\"({student_summary['Critical_Fail']/student_summary['Total_Students']:.1%}) and \"\n",
    "        f\"{student_summary['High_Achiever']} high achievers.\"\n",
    "    )\n",
    "\n",
    "    # 2. Subject Performance\n",
    "    doc.add_heading('2. Subject Performance Analysis', level=2)\n",
    "    if not subject_metrics.empty:\n",
    "        add_dataframe_to_doc(doc, subject_metrics.round(1),\n",
    "                           title=\"Subject Performance Metrics\")\n",
    "        if subject_avg_plot:\n",
    "            doc.add_picture(subject_avg_plot, width=Inches(6))\n",
    "    else:\n",
    "        doc.add_paragraph(\"No valid subject data available for analysis.\")\n",
    "\n",
    "    # 3. Student Categorization\n",
    "    doc.add_heading('3. Student Performance Categories', level=2)\n",
    "    doc.add_paragraph(\n",
    "        f\"Critical Fail (<{PASS_THRESHOLD}% in any core subject): \"\n",
    "        f\"{student_summary['Critical_Fail']} students \"\n",
    "        f\"({student_summary['Critical_Fail']/student_summary['Total_Students']:.1%})\"\n",
    "    )\n",
    "    doc.add_paragraph(\n",
    "        f\"At-Risk ({AT_RISK_LOW}-{AT_RISK_HIGH}% in any core subject): \"\n",
    "        f\"{student_summary['At_Risk']} students \"\n",
    "        f\"({student_summary['At_Risk']/student_summary['Total_Students']:.1%})\"\n",
    "    )\n",
    "    doc.add_paragraph(\n",
    "        f\"Meets Expectations (≥{MEETS_EXPECTATIONS_THRESHOLD}% in all core subjects): \"\n",
    "        f\"{student_summary['Meets_Expectations']} students\"\n",
    "    )\n",
    "    doc.add_paragraph(\n",
    "        f\"High Achievers (≥{HIGH_ACHIEVER_THRESHOLD}% in all core subjects): \"\n",
    "        f\"{student_summary['High_Achiever']} students\"\n",
    "    )\n",
    "\n",
    "    # 4. Core Subject Relationships\n",
    "    if core_corr_plot:\n",
    "        doc.add_heading('4. Core Subject Relationships', level=2)\n",
    "        doc.add_paragraph(\"Correlation matrix showing relationships between core subjects:\")\n",
    "        doc.add_picture(core_corr_plot, width=Inches(6))\n",
    "\n",
    "    # 5. Average Score Prediction\n",
    "    doc.add_heading('5. Average Score Prediction (<50 vs >=50)', level=2)\n",
    "\n",
    "    if 'accuracy' in ml_results:\n",
    "        doc.add_paragraph(\n",
    "            f\"Machine learning model trained to predict if average score will be \"\n",
    "            f\"below {PASS_THRESHOLD} with accuracy of {ml_results['accuracy']:.1%}.\"\n",
    "        )\n",
    "\n",
    "        # Add confusion matrix\n",
    "        doc.add_picture(ml_results['confusion_matrix_plot'], width=Inches(4))\n",
    "        add_dataframe_to_doc(doc, ml_results['confusion_matrix'],\n",
    "                           title=\"Confusion Matrix for Average Score Prediction\")\n",
    "\n",
    "        # Add feature importance\n",
    "        doc.add_paragraph(\"Most important subjects for predicting average score:\")\n",
    "        doc.add_picture(ml_results['feature_importance_plot'], width=Inches(6))\n",
    "\n",
    "        # Add SHAP analysis if available\n",
    "        if _HAS_SHAP and ml_results.get('shap_results') and not ml_results['shap_results'].get('error'):\n",
    "            doc.add_heading('SHAP Analysis - Model Interpretability', level=3)\n",
    "            doc.add_paragraph(\n",
    "                \"SHAP (SHapley Additive exPlanations) values show how each feature \"\n",
    "                \"contributes to the prediction for individual students:\"\n",
    "            )\n",
    "\n",
    "            # SHAP Summary Plot\n",
    "            doc.add_paragraph(\"SHAP Summary Plot - Global Feature Importance:\", style='Heading 3')\n",
    "            doc.add_picture(ml_results['shap_results']['summary_plot'], width=Inches(6))\n",
    "            doc.add_paragraph(\n",
    "                \"This plot shows both feature importance (vertical dispersion) and \"\n",
    "                \"the impact of each feature on the prediction (color). Red indicates \"\n",
    "                \"higher values push the prediction towards average <50.\"\n",
    "            )\n",
    "\n",
    "            # SHAP Force Plot\n",
    "            if ml_results['shap_results'].get('force_plot'):\n",
    "                doc.add_paragraph(\"SHAP Force Plot - Individual Prediction Explanation:\", style='Heading 3')\n",
    "                doc.add_picture(ml_results['shap_results']['force_plot'], width=Inches(6))\n",
    "                doc.add_paragraph(\n",
    "                    \"This shows how each feature contributes to a specific student's prediction. \"\n",
    "                    \"Features pushing the prediction to the right increase the likelihood of average <50.\"\n",
    "                )\n",
    "\n",
    "            # SHAP Dependence Plot\n",
    "            if ml_results['shap_results'].get('dependence_plot'):\n",
    "                doc.add_paragraph(\"SHAP Dependence Plot - Feature Relationship:\", style='Heading 3')\n",
    "                doc.add_picture(ml_results['shap_results']['dependence_plot'], width=Inches(6))\n",
    "                doc.add_paragraph(\n",
    "                    f\"This shows how the most important feature ({feature_importances.index[0]}) \"\n",
    "                    \"interacts with other features to affect the prediction.\"\n",
    "                )\n",
    "        elif not _HAS_SHAP:\n",
    "            doc.add_paragraph(\n",
    "                \"Note: Install SHAP (pip install shap) for advanced model interpretability features.\"\n",
    "            )\n",
    "\n",
    "        # Add classification report\n",
    "        cr_df = pd.DataFrame(ml_results['classification_report']).transpose()\n",
    "        add_dataframe_to_doc(doc, cr_df.round(3), title=\"Classification Report\")\n",
    "\n",
    "        # Show top at-risk students\n",
    "        if 'Avg_Score_Fail_Probability' in df.columns:\n",
    "            top_risk = df.nlargest(10, 'Avg_Score_Fail_Probability')\n",
    "            display_cols = ['Avg_Score_Fail_Probability', 'Average_Score'] + valid_subjects\n",
    "            display_cols = [c for c in display_cols if c in top_risk.columns]\n",
    "            add_dataframe_to_doc(\n",
    "                doc,\n",
    "                top_risk[display_cols].round(2),\n",
    "                title=\"Top 10 Students at Risk of Average <50\"\n",
    "            )\n",
    "    else:\n",
    "        doc.add_paragraph(ml_results.get('note', 'No model trained'))\n",
    "\n",
    "    # 6. Practical Recommendations\n",
    "    doc.add_heading('6. Practical Recommendations', level=2)\n",
    "    doc.add_paragraph(\"Targeted interventions for improving student performance:\")\n",
    "\n",
    "    # Subject-specific recommendations\n",
    "    for subject in valid_subjects[:5]:  # Show top 5 subjects\n",
    "        doc.add_heading(f\"{subject} Interventions\", level=3)\n",
    "        for rec in get_subject_recommendations(subject):\n",
    "            doc.add_paragraph(f\"• {rec}\", style='List Bullet')\n",
    "\n",
    "    # Implementation plan\n",
    "    doc.add_heading('12-Week Implementation Plan', level=3)\n",
    "    plan = [\n",
    "        \"Weeks 1-2: Diagnostic testing and student grouping\",\n",
    "        \"Weeks 3-6: Intensive remedial sessions (3x weekly)\",\n",
    "        \"Weeks 7-9: Progress monitoring and adjustment\",\n",
    "        \"Weeks 10-12: Final assessment and reporting\"\n",
    "    ]\n",
    "    for item in plan:\n",
    "        doc.add_paragraph(item, style='List Bullet')\n",
    "\n",
    "    # Save document\n",
    "    doc.save(OUTPUT_WORD)\n",
    "    print(f\"Report saved to {OUTPUT_WORD}\")\n",
    "\n",
    "# Generate the report\n",
    "generate_word_report()\n",
    "\n",
    "# ==========================================================\n",
    "# SAVE PROCESSED DATA WITH SHAP VALUES\n",
    "# ==========================================================\n",
    "\n",
    "try:\n",
    "    # Add SHAP values to dataframe if available\n",
    "    if _HAS_SHAP and shap_results and 'shap_values' in shap_results and 'explainer' in shap_results:\n",
    "        # Get SHAP values for all data\n",
    "        X_all = df[valid_subjects].fillna(df[valid_subjects].median())\n",
    "        shap_values_all = shap_results['explainer'].shap_values(X_all)\n",
    "\n",
    "        # Handle binary classification SHAP values structure for all data\n",
    "        if isinstance(shap_values_all, list) and len(shap_values_all) == 2:\n",
    "            shap_values_class1_all = shap_values_all[1]  # Use class 1 (average <50)\n",
    "        else:\n",
    "            shap_values_class1_all = shap_values_all\n",
    "\n",
    "        # Add SHAP values for each subject\n",
    "        for i, subject in enumerate(valid_subjects):\n",
    "            if i < shap_values_class1_all.shape[1]:  # Ensure we don't exceed array bounds\n",
    "                df[f'SHAP_{subject}'] = shap_values_class1_all[:, i]\n",
    "\n",
    "    df.to_excel(OUTPUT_PROCESSED, index=False)\n",
    "    print(f\"Processed data with SHAP values saved to {OUTPUT_PROCESSED}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving processed data: {e}\")\n",
    "\n",
    "print(\"Analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
